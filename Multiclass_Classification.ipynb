{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eac53e2d-d249-4553-abf8-f5350a71fba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution: Counter({0: 1096, 10: 580, 5: 579, 8: 576, 4: 573, 6: 571, 2: 569, 3: 566, 9: 547, 7: 538, 1: 30})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models\n",
    "from torchvision.models import vgg16,resnet50, MobileNetV3, Inception_V3_Weights, EfficientNet_B0_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Class names\n",
    "class_names = [\n",
    "    'animal fish',\n",
    "    'animal fish bass',\n",
    "    'fish sea_food black_sea_sprat',\n",
    "    'fish sea_food gilt_head_bream',\n",
    "    'fish sea_food hourse_mackerel',\n",
    "    'fish sea_food red_mullet',\n",
    "    'fish sea_food red_sea_bream',\n",
    "    'fish sea_food sea_bass',\n",
    "    'fish sea_food shrimp',\n",
    "    'fish sea_food striped_red_mullet',\n",
    "    'fish sea_food trout'\n",
    "]\n",
    "\n",
    "# Paths\n",
    "img_path_train = r\"C:\\Users\\achu1\\Documents\\GUVI\\Project -5\\data\\train\"\n",
    "img_path_val = r\"C:\\Users\\achu1\\Documents\\GUVI\\Project -5\\data\\val\"\n",
    "img_path_test = r\"C:\\Users\\achu1\\Documents\\GUVI\\Project -5\\data\\test\"\n",
    "\n",
    "# Transforms\n",
    "trans = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_data = datasets.ImageFolder(root=img_path_train, transform=trans)\n",
    "val_data = datasets.ImageFolder(root=img_path_val, transform=trans)\n",
    "test_data = datasets.ImageFolder(root=img_path_test, transform=trans)\n",
    "\n",
    "# DataLoaders\n",
    "train_data_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_data_loader = DataLoader(val_data, batch_size=32)\n",
    "test_data_loader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "# Print class distribution\n",
    "print(\"Class Distribution:\", Counter(train_data.targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26bf3036-3fc8-4e4e-915e-b83386892bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class_names = [\n",
    "    'animal fish',\n",
    "    'animal fish bass',\n",
    "    'fish sea_food black_sea_sprat',\n",
    "    'fish sea_food gilt_head_bream',\n",
    "    'fish sea_food hourse_mackerel',\n",
    "    'fish sea_food red_mullet',\n",
    "    'fish sea_food red_sea_bream',\n",
    "    'fish sea_food sea_bass',\n",
    "    'fish sea_food shrimp',\n",
    "    'fish sea_food striped_red_mullet',\n",
    "    'fish sea_food trout'\n",
    "]\n",
    "\n",
    "with open(\"class_names.pkl\", \"wb\") as f:\n",
    "    pickle.dump(class_names, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa00f77f-d5df-4551-a981-b3d74ddb19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "\n",
    "    nn.Conv2d(16, 32, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "\n",
    "    nn.Conv2d(32, 64, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64 * 16 * 16, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(128, len(class_names))\n",
    ").to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "model_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf0a3c46-b880-4e24-92d8-e6f17cf54fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Training Loss: 254.6331\n",
      "Epoch 2/50 - Training Loss: 114.7357\n",
      "Epoch 3/50 - Training Loss: 71.8091\n",
      "Epoch 4/50 - Training Loss: 58.5423\n",
      "Epoch 5/50 - Training Loss: 44.5056\n",
      "Epoch 6/50 - Training Loss: 34.8796\n",
      "Epoch 7/50 - Training Loss: 34.4719\n",
      "Epoch 8/50 - Training Loss: 29.0318\n",
      "Epoch 9/50 - Training Loss: 26.1810\n",
      "Epoch 10/50 - Training Loss: 18.1689\n",
      "Epoch 11/50 - Training Loss: 19.3811\n",
      "Epoch 12/50 - Training Loss: 19.4668\n",
      "Epoch 13/50 - Training Loss: 15.6842\n",
      "Epoch 14/50 - Training Loss: 15.4212\n",
      "Epoch 15/50 - Training Loss: 17.5364\n",
      "Epoch 16/50 - Training Loss: 13.9419\n",
      "Epoch 17/50 - Training Loss: 15.7775\n",
      "Epoch 18/50 - Training Loss: 12.2194\n",
      "Epoch 19/50 - Training Loss: 10.0567\n",
      "Epoch 20/50 - Training Loss: 12.9912\n",
      "Epoch 21/50 - Training Loss: 11.4044\n",
      "Epoch 22/50 - Training Loss: 9.6504\n",
      "Epoch 23/50 - Training Loss: 12.1612\n",
      "Epoch 24/50 - Training Loss: 14.0002\n",
      "Epoch 25/50 - Training Loss: 10.8381\n",
      "Epoch 26/50 - Training Loss: 7.4538\n",
      "Epoch 27/50 - Training Loss: 13.4696\n",
      "Epoch 28/50 - Training Loss: 7.4289\n",
      "Epoch 29/50 - Training Loss: 7.5155\n",
      "Epoch 30/50 - Training Loss: 5.5197\n",
      "Epoch 31/50 - Training Loss: 8.4039\n",
      "Epoch 32/50 - Training Loss: 13.9750\n",
      "Epoch 33/50 - Training Loss: 7.5507\n",
      "Epoch 34/50 - Training Loss: 6.6250\n",
      "Epoch 35/50 - Training Loss: 7.7881\n",
      "Epoch 36/50 - Training Loss: 11.2874\n",
      "Epoch 37/50 - Training Loss: 10.0067\n",
      "Epoch 38/50 - Training Loss: 8.1093\n",
      "Epoch 39/50 - Training Loss: 6.9357\n",
      "Epoch 40/50 - Training Loss: 5.2897\n",
      "Epoch 41/50 - Training Loss: 6.2050\n",
      "Epoch 42/50 - Training Loss: 10.1832\n",
      "Epoch 43/50 - Training Loss: 7.6129\n",
      "Epoch 44/50 - Training Loss: 4.3446\n",
      "Epoch 45/50 - Training Loss: 7.2358\n",
      "Epoch 46/50 - Training Loss: 7.2725\n",
      "Epoch 47/50 - Training Loss: 12.0449\n",
      "Epoch 48/50 - Training Loss: 7.3266\n",
      "Epoch 49/50 - Training Loss: 5.2271\n",
      "Epoch 50/50 - Training Loss: 2.8025\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = model_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {running_loss:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e21019c-b55d-4963-b781-b59717a7e2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 98.53%\n"
     ]
    }
   ],
   "source": [
    "  # Validate\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "        for images, labels in val_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "val_acc = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "# Function to predict class of an image\n",
    "def pred_img_class(img_path):\n",
    "    model.eval()\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    image_tensor = trans(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        predicted_class = class_names[pred.item()]\n",
    "        print(\"Predicted Class:\", predicted_class)\n",
    "        return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8600a56d-af4c-4899-8925-cb22b586865f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: fish sea_food trout\n",
      "Accuracy: 99.92%\n",
      "Accuracy: 98.99%\n",
      "Accuracy: 99.40%\n",
      "Accuracy: 99.47%\n",
      "Test Accuracy: 99.46658299341073\n"
     ]
    }
   ],
   "source": [
    "# Final accuracy calculation\n",
    "def calculate_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    A1 = 100 * correct / total\n",
    "    print(f\"Accuracy: {A1:.2f}%\")\n",
    "    return A1 #Accuracy\n",
    "\n",
    "# Try predicting an image\n",
    "pred_img_class(r\"C:\\Users\\achu1\\Downloads\\AIML Tamil Recordings\\Project orientation\\Project 5-Multiclass Fish Image Classification\\Dataset\\images.cv_jzk6llhf18tm3k0kyttxz\\data\\train\\fish sea_food trout\\1RNWEVD9TZH3.jpg\")\n",
    "\n",
    "# Final accuracy on train/val/test\n",
    "calculate_accuracy(model, train_data_loader)\n",
    "calculate_accuracy(model, val_data_loader)\n",
    "calculate_accuracy(model, test_data_loader)\n",
    "\n",
    "A1 = calculate_accuracy(model, test_data_loader)\n",
    "print(\"Test Accuracy:\", A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "893e6501-159d-4766-aad9-3ef10c46f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, \"fish_classifier_model.pth\") \n",
    "torch.save(model, \"cnn_model.pth\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83bea75d-f254-47ec-89eb-852319612d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: fish sea_food hourse_mackerel\n",
      "Accuracy: 99.95%\n",
      "Accuracy: 98.53%\n",
      "Accuracy: 99.50%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99.49796046438657"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EX\n",
    "pred_img_class(r\"C:\\Users\\achu1\\Downloads\\AIML Tamil Recordings\\Project orientation\\Project 5-Multiclass Fish Image Classification\\Dataset\\images.cv_jzk6llhf18tm3k0kyttxz\\data\\train\\fish sea_food hourse_mackerel\\GEV2Q44R5ENG.jpg\")\n",
    "\n",
    "# Final accuracy on train/val/test\n",
    "calculate_accuracy(model, train_data_loader)\n",
    "calculate_accuracy(model, val_data_loader)\n",
    "calculate_accuracy(model, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4e13294-3575-4d1e-9264-0d1fd5e65b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: fish sea_food shrimp\n",
      "Accuracy: 99.94%\n",
      "Accuracy: 98.63%\n",
      "Accuracy: 99.50%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99.49796046438657"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_img_class(r\"C:\\Users\\achu1\\Downloads\\AIML Tamil Recordings\\Project orientation\\Project 5-Multiclass Fish Image Classification\\Dataset\\images.cv_jzk6llhf18tm3k0kyttxz\\data\\test\\fish sea_food shrimp\\6ZYXHRU8APXV.jpg\")\n",
    "\n",
    "# Final accuracy on train/val/test\n",
    "calculate_accuracy(model, train_data_loader)\n",
    "calculate_accuracy(model, val_data_loader)\n",
    "calculate_accuracy(model, test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d2d5ce-6657-4f9d-a797-578e4770e216",
   "metadata": {},
   "source": [
    "## VGG16:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92003eeb-4d14-4047-9e3e-b655d678e7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achu1\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\achu1\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "vgg = vgg16(pretrained=True)\n",
    "for param in vgg.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the classifier\n",
    "vgg.classifier = nn.Sequential(\n",
    "    nn.Linear(25088, 512),  # 25088 = 512 * 7 * 7 (flattened VGG16 output)\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, len(class_names))  # your number of classes\n",
    ")\n",
    "\n",
    "vgg = vgg.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vgg.classifier.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b938b6c5-4586-4432-8bb9-dab246a013f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Training Loss: 109.6468\n",
      "Epoch 2/20 - Training Loss: 40.3600\n",
      "Epoch 3/20 - Training Loss: 36.9347\n",
      "Epoch 4/20 - Training Loss: 36.2121\n",
      "Epoch 5/20 - Training Loss: 28.0188\n",
      "Epoch 6/20 - Training Loss: 24.9327\n",
      "Epoch 7/20 - Training Loss: 18.4582\n",
      "Epoch 8/20 - Training Loss: 23.3316\n",
      "Epoch 9/20 - Training Loss: 25.6504\n",
      "Epoch 10/20 - Training Loss: 28.1144\n",
      "Epoch 11/20 - Training Loss: 20.5447\n",
      "Epoch 12/20 - Training Loss: 22.2787\n",
      "Epoch 13/20 - Training Loss: 23.0533\n",
      "Epoch 14/20 - Training Loss: 21.4129\n",
      "Epoch 15/20 - Training Loss: 25.2599\n",
      "Epoch 16/20 - Training Loss: 20.8212\n",
      "Epoch 17/20 - Training Loss: 21.9612\n",
      "Epoch 18/20 - Training Loss: 17.2629\n",
      "Epoch 19/20 - Training Loss: 31.1507\n",
      "Epoch 20/20 - Training Loss: 30.5610\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    vgg.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vgg(images)  # not vgg.classifier(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {running_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10034ff8-d291-4495-ae6b-764717a779d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 97.99%\n"
     ]
    }
   ],
   "source": [
    "  # Validate\n",
    "vgg.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "        for images, labels in val_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = vgg(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "val_acc = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "# Function to predict class of an image\n",
    "def pred_img_class(img_path):\n",
    "    vgg.eval()\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    image_tensor = trans(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = vgg(image_tensor)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        predicted_class = class_names[pred.item()]\n",
    "        print(\"Predicted Class:\", predicted_class)\n",
    "        return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8825af63-a688-4b75-b63e-a36c89c40925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: fish sea_food trout\n",
      "Accuracy: 98.94%\n",
      "Accuracy: 98.99%\n",
      "Accuracy: 99.02%\n",
      "Accuracy: 98.97%\n",
      "Test Accuracy: 98.9718875502008\n"
     ]
    }
   ],
   "source": [
    "# Final accuracy calculation\n",
    "def calculate_accuracy(model, data_loader):\n",
    "    vgg.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in train_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = vgg(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    A2 = 100 * correct / total\n",
    "    print(f\"Accuracy: {A2:.2f}%\")\n",
    "    return A2\n",
    "\n",
    "# Try predicting an image\n",
    "pred_img_class(r\"C:\\Users\\achu1\\Downloads\\AIML Tamil Recordings\\Project orientation\\Project 5-Multiclass Fish Image Classification\\Dataset\\images.cv_jzk6llhf18tm3k0kyttxz\\data\\train\\fish sea_food trout\\1RNWEVD9TZH3.jpg\")\n",
    "\n",
    "# Final accuracy on train/val/test\n",
    "calculate_accuracy(vgg, train_data_loader)\n",
    "calculate_accuracy(vgg, val_data_loader)\n",
    "calculate_accuracy(vgg, test_data_loader)\n",
    "\n",
    "A2 = calculate_accuracy(vgg, test_data_loader)\n",
    "print(\"Test Accuracy:\", A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c77b2c05-fe7c-4fd3-9c46-ff99a8de00be",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vgg, \"vgg16_model.pth\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b8ecc0-b63c-4d43-a441-e03989d75d58",
   "metadata": {},
   "source": [
    "## Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bcb7035e-e20a-4af3-9494-76fdad85cba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achu1\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load ResNet50\n",
    "resnet = resnet50(pretrained=True)\n",
    "\n",
    "# Freeze feature extractor layers\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final fully connected layer\n",
    "resnet.fc = nn.Sequential(\n",
    "    nn.Linear(resnet.fc.in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, len(class_names))\n",
    ")\n",
    "\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06e8498c-5db8-4ee3-878f-4a30a705a6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Training Loss: 124.0024\n",
      "Epoch 2/20 - Training Loss: 60.4665\n",
      "Epoch 3/20 - Training Loss: 55.7706\n",
      "Epoch 4/20 - Training Loss: 45.9367\n",
      "Epoch 5/20 - Training Loss: 38.5934\n",
      "Epoch 6/20 - Training Loss: 39.1693\n",
      "Epoch 7/20 - Training Loss: 41.1560\n",
      "Epoch 8/20 - Training Loss: 36.9266\n",
      "Epoch 9/20 - Training Loss: 36.6242\n",
      "Epoch 10/20 - Training Loss: 36.4609\n",
      "Epoch 11/20 - Training Loss: 33.2191\n",
      "Epoch 12/20 - Training Loss: 31.2902\n",
      "Epoch 13/20 - Training Loss: 31.8092\n",
      "Epoch 14/20 - Training Loss: 31.0786\n",
      "Epoch 15/20 - Training Loss: 30.4065\n",
      "Epoch 16/20 - Training Loss: 27.1667\n",
      "Epoch 17/20 - Training Loss: 32.6478\n",
      "Epoch 18/20 - Training Loss: 27.5814\n",
      "Epoch 19/20 - Training Loss: 27.2148\n",
      "Epoch 20/20 - Training Loss: 30.1436\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    resnet.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {running_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4001288a-c6b1-4347-8c0b-037f80b2d700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 98.57%\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "resnet.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "        for images, labels in train_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = resnet(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "val_acc = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cdd1821e-7e59-4d6e-9389-96d9ae7f34bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: fish sea_food trout\n",
      "Accuracy: 98.59%\n",
      "Accuracy: 97.44%\n",
      "Accuracy: 97.30%\n",
      "Accuracy: 97.14%\n",
      "Test Accuracy: 97.14465014119862\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prediction function\n",
    "def pred_img_class(img_path):\n",
    "    resnet.eval()\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    image_tensor = trans(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = resnet(image_tensor)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        predicted_class = class_names[pred.item()]\n",
    "        print(\"Predicted Class:\", predicted_class)\n",
    "        return predicted_class\n",
    "\n",
    "# Accuracy function\n",
    "def calculate_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    A3 = 100 * correct / total\n",
    "    print(f\"Accuracy: {A3:.2f}%\")\n",
    "    return A3\n",
    "\n",
    "\n",
    "# Example usage\n",
    "pred_img_class(r\"C:\\Users\\achu1\\Downloads\\AIML Tamil Recordings\\Project orientation\\Project 5-Multiclass Fish Image Classification\\Dataset\\images.cv_jzk6llhf18tm3k0kyttxz\\data\\train\\fish sea_food trout\\1RNWEVD9TZH3.jpg\")\n",
    "\n",
    "calculate_accuracy(resnet, train_data_loader)\n",
    "calculate_accuracy(resnet, val_data_loader)\n",
    "calculate_accuracy(resnet, test_data_loader)\n",
    "\n",
    "A3 = calculate_accuracy(resnet, test_data_loader)\n",
    "print(\"Test Accuracy:\", A3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "755482a8-525d-4c3d-8ad4-5c0c6802bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet, \"resnet_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8445ad1-710f-4648-bb78-68e857ddcc47",
   "metadata": {},
   "source": [
    "## Mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1902276-c597-4103-b3f2-d7056ba8a9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achu1\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load MobileNetV3 (large version)\n",
    "mobilenet = models.mobilenet_v3_large(pretrained=True)\n",
    "\n",
    "# Freeze feature extractor\n",
    "for param in mobilenet.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the classifier\n",
    "mobilenet.classifier = nn.Sequential(\n",
    "    nn.Linear(mobilenet.classifier[0].in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, len(class_names))\n",
    ")\n",
    "\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mobilenet.classifier.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "993bb801-d128-4207-89bb-52c03a0fa4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Training Loss: 102.5968\n",
      "Epoch 2/20 - Training Loss: 44.8658\n",
      "Epoch 3/20 - Training Loss: 38.3672\n",
      "Epoch 4/20 - Training Loss: 34.1510\n",
      "Epoch 5/20 - Training Loss: 28.9037\n",
      "Epoch 6/20 - Training Loss: 24.7364\n",
      "Epoch 7/20 - Training Loss: 22.1588\n",
      "Epoch 8/20 - Training Loss: 20.9949\n",
      "Epoch 9/20 - Training Loss: 21.8207\n",
      "Epoch 10/20 - Training Loss: 19.3542\n",
      "Epoch 11/20 - Training Loss: 20.2246\n",
      "Epoch 12/20 - Training Loss: 17.9877\n",
      "Epoch 13/20 - Training Loss: 17.3941\n",
      "Epoch 14/20 - Training Loss: 16.8036\n",
      "Epoch 15/20 - Training Loss: 17.0379\n",
      "Epoch 16/20 - Training Loss: 16.6660\n",
      "Epoch 17/20 - Training Loss: 16.3851\n",
      "Epoch 18/20 - Training Loss: 15.0288\n",
      "Epoch 19/20 - Training Loss: 15.3170\n",
      "Epoch 20/20 - Training Loss: 13.7174\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    mobilenet.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mobilenet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {running_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7812285f-167e-4fdf-a50a-821fdd6efba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 97.34%\n"
     ]
    }
   ],
   "source": [
    "    # Validation\n",
    "    mobilenet.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = mobilenet(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c98b2ebe-e8ed-4673-b27d-2d5e04b17b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: fish sea_food trout\n",
      "Accuracy: 98.34%\n",
      "Accuracy: 98.46%\n",
      "Accuracy: 98.43%\n",
      "Accuracy: 98.31%\n",
      "Test Accuracy: 98.30561656730468\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Prediction function\n",
    "def pred_img_class(img_path):\n",
    "    mobilenet.eval()\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    image_tensor = trans(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = mobilenet(image_tensor)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        predicted_class = class_names[pred.item()]\n",
    "        print(\"Predicted Class:\", predicted_class)\n",
    "        return predicted_class\n",
    "\n",
    "\n",
    "# Accuracy function\n",
    "def calculate_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    A4 = 100 * correct / total\n",
    "    print(f\"Accuracy: {A4:.2f}%\")\n",
    "    return A4\n",
    "\n",
    "\n",
    "# Test it\n",
    "pred_img_class(r\"C:\\Users\\achu1\\Downloads\\AIML Tamil Recordings\\Project orientation\\Project 5-Multiclass Fish Image Classification\\Dataset\\images.cv_jzk6llhf18tm3k0kyttxz\\data\\train\\fish sea_food trout\\1RNWEVD9TZH3.jpg\")\n",
    "\n",
    "calculate_accuracy(mobilenet, train_data_loader)\n",
    "calculate_accuracy(mobilenet, val_data_loader)\n",
    "calculate_accuracy(mobilenet, test_data_loader)\n",
    "\n",
    "A4 = calculate_accuracy(mobilenet, test_data_loader)\n",
    "print(\"Test Accuracy:\", A4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6aa20301-83bd-452c-b567-43ef9834de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mobilenet, \"mobilenet.pth\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1582c091-00c2-4521-a47b-328358d839e5",
   "metadata": {},
   "source": [
    "## InceptionV3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05145386-2b8c-4ab6-89a3-d9df82d9ea89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achu1\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # <- Resize to 299x299\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_data = datasets.ImageFolder(root=img_path_train, transform=transform)\n",
    "val_data = datasets.ImageFolder(root=img_path_val, transform=transform)\n",
    "test_data = datasets.ImageFolder(root=img_path_test, transform=transform)\n",
    "\n",
    "# DataLoaders\n",
    "train_data_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_data_loader = DataLoader(val_data, batch_size=32)\n",
    "test_data_loader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "# Load Inception V3 (pretrained)\n",
    "inception = models.inception_v3(pretrained=True)\n",
    "\n",
    "# Freeze all layers except the final classifier\n",
    "for param in inception.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the classifier for our task\n",
    "inception.AuxLogits.fc = nn.Sequential(\n",
    "    nn.Linear(inception.AuxLogits.fc.in_features, len(class_names))\n",
    ")\n",
    "\n",
    "inception.fc = nn.Sequential(\n",
    "    nn.Linear(inception.fc.in_features, len(class_names))\n",
    ")\n",
    "\n",
    "# Move model to device (GPU or CPU)\n",
    "inception = inception.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(inception.fc.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ca5c5b8-2194-4c46-b9c0-de42ae1f6f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Training Loss: 376.2054\n",
      "Epoch 2/20 - Training Loss: 261.5044\n",
      "Epoch 3/20 - Training Loss: 239.1089\n",
      "Epoch 4/20 - Training Loss: 229.9713\n",
      "Epoch 5/20 - Training Loss: 225.0884\n",
      "Epoch 6/20 - Training Loss: 221.4644\n",
      "Epoch 7/20 - Training Loss: 219.4487\n",
      "Epoch 8/20 - Training Loss: 217.8293\n",
      "Epoch 9/20 - Training Loss: 214.2628\n",
      "Epoch 10/20 - Training Loss: 214.9523\n",
      "Epoch 11/20 - Training Loss: 213.3569\n",
      "Epoch 12/20 - Training Loss: 210.6734\n",
      "Epoch 13/20 - Training Loss: 212.2096\n",
      "Epoch 14/20 - Training Loss: 210.5636\n",
      "Epoch 15/20 - Training Loss: 210.8212\n",
      "Epoch 16/20 - Training Loss: 208.7155\n",
      "Epoch 17/20 - Training Loss: 208.3593\n",
      "Epoch 18/20 - Training Loss: 207.7360\n",
      "Epoch 19/20 - Training Loss: 208.7554\n",
      "Epoch 20/20 - Training Loss: 208.4075\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    inception.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs,aux_outputs = inception(images)\n",
    "        loss1 = criterion(outputs, labels)\n",
    "        loss2 = criterion(aux_outputs, labels)\n",
    "        loss = loss1 + 0.4 * loss2 \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {running_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a8d08469-4399-4496-ab5b-68989f5cfdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 98.81%\n"
     ]
    }
   ],
   "source": [
    "    # Validation\n",
    "inception.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "        for images, labels in val_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = inception(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "24e8d9f1-6a75-4b12-b9db-3c0803b53a96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: fish sea_food trout\n",
      "Accuracy: 99.22%\n",
      "Accuracy: 99.22%\n",
      "Accuracy: 99.22%\n",
      "Accuracy: 99.22%\n",
      "Test Accuracy: 99.21556322560401\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Prediction function\n",
    "def pred_img_class(img_path):\n",
    "    inception.eval()\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    image_tensor = trans(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = inception(image_tensor)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        predicted_class = class_names[pred.item()]\n",
    "        print(\"Predicted Class:\", predicted_class)\n",
    "        return predicted_class\n",
    "\n",
    "\n",
    "# Accuracy function\n",
    "def calculate_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    A5 = 100 * correct / total\n",
    "    print(f\"Accuracy: {A5:.2f}%\")\n",
    "    return A5\n",
    "\n",
    "\n",
    "# Test the model with an image\n",
    "pred_img_class(r\"C:\\Users\\achu1\\Downloads\\AIML Tamil Recordings\\Project orientation\\Project 5-Multiclass Fish Image Classification\\Dataset\\images.cv_jzk6llhf18tm3k0kyttxz\\data\\train\\fish sea_food trout\\1RNWEVD9TZH3.jpg\")\n",
    "\n",
    "# Final accuracy on train/val/test\n",
    "calculate_accuracy(inception, train_data_loader)\n",
    "calculate_accuracy(inception, val_data_loader)\n",
    "calculate_accuracy(inception, test_data_loader)\n",
    "\n",
    "A5 = calculate_accuracy(inception, test_data_loader)\n",
    "print(\"Test Accuracy:\", A5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5dff177-3437-4e69-8b55-78181ddd4998",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(inception, \"inception.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a2f644-1074-439f-8159-c6ca0a10a165",
   "metadata": {},
   "source": [
    "## EfficientNet0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cbe31b60-f05b-46ff-9820-363fa2bbf4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3938a36e-c030-4f25-9a7d-296a55c33594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# Transforms\n",
    "trans = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_data = datasets.ImageFolder(root=img_path_train, transform=trans)\n",
    "val_data = datasets.ImageFolder(root=img_path_val, transform=trans)\n",
    "test_data = datasets.ImageFolder(root=img_path_test, transform=trans)\n",
    "\n",
    "# DataLoaders\n",
    "train_data_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_data_loader = DataLoader(val_data, batch_size=32)\n",
    "test_data_loader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "# Load EfficientNetB0 (pretrained)\n",
    "efficientnet = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "\n",
    "# Freeze all layers except the final classifier\n",
    "for param in efficientnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the classifier for our task\n",
    "efficientnet._fc = nn.Sequential(\n",
    "    nn.Linear(efficientnet._fc.in_features, len(class_names))\n",
    ")\n",
    "\n",
    "# Move model to device (GPU or CPU)\n",
    "efficientnet = efficientnet.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(efficientnet._fc.parameters(), lr=0.001)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bdc4972d-d7d7-49bc-821d-da33038bf12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Training Loss: 183.8495\n",
      "Epoch 2/20 - Training Loss: 80.2207\n",
      "Epoch 3/20 - Training Loss: 62.4017\n",
      "Epoch 4/20 - Training Loss: 52.3834\n",
      "Epoch 5/20 - Training Loss: 44.3826\n",
      "Epoch 6/20 - Training Loss: 40.7085\n",
      "Epoch 7/20 - Training Loss: 38.9286\n",
      "Epoch 8/20 - Training Loss: 35.9660\n",
      "Epoch 9/20 - Training Loss: 34.0754\n",
      "Epoch 10/20 - Training Loss: 34.7642\n",
      "Epoch 11/20 - Training Loss: 32.3003\n",
      "Epoch 12/20 - Training Loss: 31.7812\n",
      "Epoch 13/20 - Training Loss: 29.3900\n",
      "Epoch 14/20 - Training Loss: 28.8326\n",
      "Epoch 15/20 - Training Loss: 27.3397\n",
      "Epoch 16/20 - Training Loss: 26.2493\n",
      "Epoch 17/20 - Training Loss: 26.7187\n",
      "Epoch 18/20 - Training Loss: 26.9549\n",
      "Epoch 19/20 - Training Loss: 25.1639\n",
      "Epoch 20/20 - Training Loss: 26.2793\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    efficientnet.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = efficientnet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {running_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3e1b228b-690e-48b6-b9ce-949e7f09184f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 97.53%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Validation\n",
    "    efficientnet.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = efficientnet(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dbf4f781-a68a-4884-b601-c9e03d9cd1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: fish sea_food trout\n",
      "Accuracy: 98.274%\n",
      "Accuracy: 98.117%\n",
      "Accuracy: 98.023%\n",
      "Accuracy: 98.055%\n",
      "Test Accuracy: 98.05459679949796\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Prediction function\n",
    "def pred_img_class(img_path):\n",
    "    efficientnet.eval()\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    image_tensor = trans(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = efficientnet(image_tensor)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        predicted_class = class_names[pred.item()]\n",
    "        print(\"Predicted Class:\", predicted_class)\n",
    "        return predicted_class\n",
    "\n",
    "# Accuracy function\n",
    "def calculate_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    A6 = 100 * correct / total\n",
    "    print(f\"Accuracy: {A6:.3f}%\")\n",
    "    return A6\n",
    "\n",
    "# Test the model with an image\n",
    "pred_img_class(r\"C:\\Users\\achu1\\Downloads\\AIML Tamil Recordings\\Project orientation\\Project 5-Multiclass Fish Image Classification\\Dataset\\images.cv_jzk6llhf18tm3k0kyttxz\\data\\train\\fish sea_food trout\\1RNWEVD9TZH3.jpg\")\n",
    "\n",
    "# Final accuracy on train/val/test\n",
    "calculate_accuracy(efficientnet, train_data_loader)\n",
    "calculate_accuracy(efficientnet, val_data_loader)\n",
    "calculate_accuracy(efficientnet, test_data_loader)\n",
    "A6 = calculate_accuracy(efficientnet, test_data_loader)\n",
    "print(\"Test Accuracy:\", A6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea15510d-4238-4118-80ff-ba59e5784ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(efficientnet, \"efficient.pth\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cc10cdff-0905-4475-976b-d124a3dd012d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.21556322560401"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd94bd4-c64a-4ef0-b88e-683f2e10d9d5",
   "metadata": {},
   "source": [
    "## Saving Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5502ac45-b690-4c9c-9b45-02fb6cfecbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best Model: CNN with Accuracy: 99.47%\n",
      "✅ Saved the best model as best_fish_model.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_accuracies = {\n",
    "    \"CNN\": A1,\n",
    "    \"VGG16\": A2,\n",
    "    \"ResNet50\": A3,\n",
    "    \"MobileNetV2\": A4,\n",
    "    \"InceptionV3\": A5,\n",
    "    \"EfficientNetB0\": A6,\n",
    "}\n",
    "\n",
    "# Find best model name\n",
    "best_model_name = max(model_accuracies, key=model_accuracies.get)\n",
    "print(f\"✅ Best Model: {best_model_name} with Accuracy: {model_accuracies[best_model_name]:.2f}%\")\n",
    "\n",
    "# Now load the best model (if models already saved separately)\n",
    "model_path_map = {\n",
    "    \"CNN\": \"cnn_model.pth\",\n",
    "    \"VGG16\": \"vgg16_model.pth\",\n",
    "    \"ResNet50\": \"resnet_model.pth\",\n",
    "    \"MobileNetV2\": \"mobilenet.pth\",\n",
    "    \"InceptionV3\": \"inception.pth\",\n",
    "    \"EfficientNetB0\": \"efficient.pth\",\n",
    "}\n",
    "\n",
    "# Load the actual model file\n",
    "best_model = torch.load(model_path_map[best_model_name],map_location=device, weights_only=False)\n",
    "\n",
    "# Save best model under new name\n",
    "torch.save(best_model, \"best_fish_model.pth\")\n",
    "print(\"✅ Saved the best model as best_fish_model.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c0e066-710e-4d98-98d4-8a0646315731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
